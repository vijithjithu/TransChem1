CV_flag: True                                     # whether to use cross-validation
add_vocab_flag: True                             # whether to add supplementary vocab
LLRD_flag: True                                    # whether to use layer-wise lr decay
aug_flag: True
                              # whether to apply data augmentation
model_indicator: 'pretrain'                        # whether to use pretrained model
aug_indicator:    5                                  # number of augmentation per SMILES. If no limitation, assign the indicator with None (leave it blank).

vocab_sup_file: 'data/vocab/vocab_sup_PE_II.csv'                          # supplementary vocab file path
dataset_file: 'data/new_feat_Li.csv'                                          # dataset_file path                    
model_path: 'ckpt/pretrain.pt'                                            # pretrain model path
save_path: 'ckpt/PE_II_train.pt'                     # checkpoint path
best_model_path: 'ckpt/PE_II_best_model.pt'          # best model save path

fold_seed: [79]
test_size: 0.2                                      # test_data size of train_test split
k: 5                                                # k-fold cross-validation
blocksize: 411                                      # max length of sequences after tokenization
batch_size: 16                                    # batch size
num_epochs: 40                                      # total number of epochs
warmup_ratio: 0.1                                   # warmup ratio
drop_rate: 0.1                                      # dropout
lr_rate: 0.00005                                     # initial lr for LLRD and pretrained model lr otherwise   
lr_rate_reg: 0.00005                                 # regressor lr if LLRD is not used
weight_decay: 0.00001
hidden_dropout_prob: 0.1                            # hidden layer dropout
attention_probs_dropout_prob: 0.1                   # attention dropout
tolerance: 100                                      # tolerance of no improvement in performance before early stop
num_workers: 8                                      # number of workers when loading data